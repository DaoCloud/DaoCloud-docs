# Nvidia 征服最新的 AI 测试

**GPU 制造商在图神经网络和 LLM 微调的新 MLPerf 基准测试中名列前茅**

> 原文转载自 [spectrum.ieee.org](https://spectrum.ieee.org/mlperf-nvidia-conquers)

!!! info "NVIDIA"

    Nvidia 在 AI 训练的 GPU 系统方面的主导地位在最新的 MLPerf 一组 AI 基准测试中继续保持。

![nvidia logo](../images/nvidia.jpg)

多年来，Nvidia 一直主导着许多机器学习基准测试，如今又在其腰带上增加了两个新的徽章。

[MLPerf](https://mlcommons.org/benchmarks/training/)，有时被称为“机器学习奥林匹克”的 AI 基准测试套件，
发布了一组新的训练测试，以帮助更好地在竞争计算机系统之间进行苹果对苹果的比较。
MLPerf 的新测试之一涉及[大型语言模型](https://spectrum.ieee.org/tag/llms)的微调，
这一过程是对现有的训练模型进行一些专门知识的再训练，使其适合特定用途。
另一个测试则是针对[图神经网络](https://spectrum.ieee.org/machine-learning-in-physics)，
这种机器学习类型用于一些文献数据库、金融系统中的欺诈检测和社交网络。

即使有了这些新增项以及使用 [Google](https://cloud.google.com/tpu) 和
[Intel](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html) AI
加速器的计算机的参与，由
[Nvidia 的 Hopper 架构](https://spectrum.ieee.org/nvidias-next-gpu-shows-that-transformers-are-transforming-ai)驱动的系统再次在结果中占据主导地位。
一个包含 11,616 个 Nvidia H100 GPU 的系统——迄今为止最大的集合——在九个基准测试中均名列前茅，并在其中五个（包括两个新基准测试）中创下了记录。

!!! quote "DAVE SALVATOR, NVIDIA"

    如果你只是把硬件堆上去，不一定会有改进。

这个 11,616-H100 系统是“我们做过的最大系统，”Nvidia 加速计算产品总监 [Dave Salvator](https://www.linkedin.com/in/davesalvator/)说。它在
[GPT-3 训练试验](https://spectrum.ieee.org/large-language-models-training-benchmark)中以不到 3.5 分钟的时间打破了记录。相比之下，一个 512-GPU 系统大约用了 51 分钟。（注意，[GPT-3](https://spectrum.ieee.org/tag/gpt-3) 任务并不是完整的训练，这可能需要数周时间并花费数百万美元。相反，计算机在数据的代表性部分进行训练，在一个商定的点上远未完成。）

与 Nvidia 去年在 [GPT-3](https://spectrum.ieee.org/tag/gpt-3) 上的最大参赛者——一个 3,584 H100 计算机相比，3.5 分钟的结果代表了 3.2 倍的改进。你可能会认为这只是由于这些系统规模的不同，但在 AI 计算中情况并非总是如此。

!!! quote "DAVE SALVATOR, NVIDIA"

    如果你只是把硬件堆上去，不一定会有改进。我们基本上实现了线性扩展。
    这代表了我们工程团队的巨大成就。

竞争对手们也在逐渐接近线性扩展。这一轮中，[Intel](https://spectrum.ieee.org/tag/intel) 部署了一个使用 1,024 个 GPU 的系统，
在 67 分钟内完成了 GPT-3 任务，而六个月前一个四分之一大小的计算机用了 224 分钟。Google 最大的 GPT-3 参赛者使用了 12 倍数量的
[TPU v5p](https://cloud.google.com/tpu/docs/v5p) 加速器，其任务执行速度是其最小参赛者的九倍。

Salvator 说，线性扩展对于即将到来的拥有 100,000 个 GPU 或更多的“AI 工厂”尤为重要。他表示，
今年将上线一个这样的数据中心，另一个使用 Nvidia 下一代架构 [Blackwell](https://spectrum.ieee.org/nvidia-blackwell) 的数据中心将在 2025 年启动。

## Nvidia 的连续胜利

尽管使用了与去年相同的 Hopper 架构，Nvidia 仍继续提升训练时间。Salvator 说，这完全归功于软件改进。

!!! quote "DAVE SALVATOR, NVIDIA"

    通常，在新架构发布后，我们会从软件中获得 2-2.5 倍的提升。

![minutes to train](../images/minute.png)

在 GPT-3 训练中，Nvidia 从 [2023 年 6 月的 MLPerf 基准测试](https://spectrum.ieee.org/large-language-models-training-benchmark)中记录了 27% 的改进。
Salvator 说，背后有几个软件变更。例如，Nvidia 工程师通过减少 8 位和 16 位数字之间不必要的转换并更好地定位神经网络的哪些层可以使用较低精度数字格式，优化了 Hopper 使用不太精确的 8 位浮点运算。他们还找到了更智能的方式来调整每个芯片计算引擎的功率预算，并加速了 GPU 之间的通信，Salvator 将其比作“在烤面包机里涂黄油”。

此外，该公司还实施了一种称为[闪存注意力](https://github.com/Dao-AILab/flash-attention)的方案。这种算法由 Samba Nova 创始人 Chris Re 的斯坦福大学实验室发明，通过最小化写入内存的操作来加速变压器网络。当它首次出现在 MLPerf 基准测试中时，闪存注意力将训练时间减少了多达 10%。（Intel 也使用了一个版本的闪存注意力，但不是用于 GPT-3，而是用于新基准测试之一，微调。）

通过其他软件和网络技巧，Nvidia 在文本到图像测试中实现了 80% 的速度提升，相比其 2023 年 11 月的提交。

## 新基准测试

MLPerf 增加了新的基准测试并升级了旧的，以保持与 AI 行业动态的相关性。今年新增了微调和图神经网络。

微调是对已经训练好的 LLM 进行专门化，使其适用于特定领域。例如，Nvidia 将一个已经训练好的 430 亿参数模型训练在 GPU 制造商的设计文件和文档上，以创建
[ChipNeMo，一种旨在提高其芯片设计师生产力的 AI](https://spectrum.ieee.org/ai-for-engineering)。
当时，该公司的首席技术官 Bill Dally 表示，训练 LLM 就像给它一个文科教育，而微调就像送它去读研究生。

MLPerf 基准测试采用预训练的 Llama-2-70B 模型，并要求系统使用[政府文件数据集](https://huggingface.co/datasets/tau/scrolls)对其进行微调，以生成更准确的文件摘要为目标。

有几种方法可以进行微调。MLPerf 选择了一种称为低秩适应（LoRA）的方法。这种方法最终只训练了 LLM 参数的一小部分，根据组织的说法，与其他方法相比，对硬件的负担减少了 3 倍，并减少了内存和存储的使用。

另一个新基准测试涉及[图神经网络（GNN）](https://spectrum.ieee.org/machine-learning-in-physics)。这些网络用于可以表示为一组非常大的相互连接节点的问题，例如社交网络或推荐系统。与其他 AI 任务相比，GNN 需要计算机节点之间大量的通信。

基准测试在一个显示学术作者、论文和机构关系的数据库上训练了 GNN——一个有 5.47 亿个节点和 58 亿条边的图。然后对神经网络进行训练，以预测图中每个节点的正确标签。

## 未来的竞争

2025 年的训练轮次可能会看到来自[AMD](https://spectrum.ieee.org/tag/amd)、Intel 和 Nvidia 的新加速器的正面对决。
[AMD 的 MI300 系列](https://spectrum.ieee.org/amd-mi300)约在六个月前推出，增强内存的升级版 MI325x
[计划于 2024 年底](https://www.hpcwire.com/2024/06/03/amd-clears-up-messy-gpu-roadmap-upgrades-chips-annually/)推出，
下一代 MI350 计划在 2025 年推出。[Intel 表示其 Gaudi 3](https://spectrum.ieee.org/intel-gaudi-3)将于今年晚些时候向计算机制造商普遍提供，
并将在 MLPerf 即将进行的推理基准测试中出现。Intel 高管表示，新芯片有能力在训练 LLM 方面击败 H100。但胜利可能是短暂的，
因为 Nvidia 已经揭开了新架构 [Blackwell](https://spectrum.ieee.org/nvidia-blackwell) 的面纱，计划于今年晚些时候推出。
