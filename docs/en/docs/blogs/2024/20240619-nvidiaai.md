# Nvidia Conquers Latest AI Tests

**GPU maker tops new MLPerf benchmarks on graph neural nets and LLM fine-tuning**

> Source from [spectrum.ieee.org](https://spectrum.ieee.org/mlperf-nvidia-conquers)

!!! info "NVIDIA"

    Nvidia's predominance in GPU systems for AI training continues in the latest MLPerf set of AI benchmarks.

![nvidia logo](../images/nvidia.jpg)

For years, Nvidia has dominated many machine learning benchmarks, and now there are two more notches in its belt.

[MLPerf](https://mlcommons.org/benchmarks/training/), the AI benchmarking suite sometimes called “the Olympics of machine learning,” has released a new set of training tests to help make more and better apples-to-apples comparisons between competing computer systems. One of MLPerf’s new tests concerns fine-tuning of [large language models](https://spectrum.ieee.org/tag/llms), a process that takes an existing trained model and trains it a bit more with specialized knowledge to make it fit for a particular purpose. The other is for [graph neural networks](https://spectrum.ieee.org/machine-learning-in-physics), a type of machine learning behind some literature databases, fraud detection in financial systems, and social networks.

Even with the additions and the participation of computers using [Google’s](https://cloud.google.com/tpu) and [Intel’s](https://www.intel.com/content/www/us/en/products/details/processors/ai-accelerators/gaudi-overview.html) AI accelerators, systems powered by [Nvidia’s Hopper architecture](https://spectrum.ieee.org/nvidias-next-gpu-shows-that-transformers-are-transforming-ai) dominated the results once again. One system that included 11,616 Nvidia H100 GPUs—the largest collection yet—topped each of the nine benchmarks, setting records in five of them (including the two new benchmarks).

!!! quote "DAVE SALVATOR, NVIDIA"

    If you just throw hardware at the problem, it’s not a given that you’re going to improve.

The 11,616-H100 system is “the biggest we’ve ever done,” says [Dave Salvator](https://www.linkedin.com/in/davesalvator/), director of accelerated computing products at Nvidia. It smashed through the [GPT-3 training trial](https://spectrum.ieee.org/large-language-models-training-benchmark) in less than 3.5 minutes. A 512-GPU system, for comparison, took about 51 minutes. (Note that the [GPT-3](https://spectrum.ieee.org/tag/gpt-3) task is not a full training, which could take weeks and cost millions of dollars. Instead, the computers train on a representative portion of the data, at an agreed-upon point well before completion.)

Compared to Nvidia’s largest entrant on [GPT-3](https://spectrum.ieee.org/tag/gpt-3) last year, a 3,584 H100 computer, the 3.5-minute result represents a 3.2-fold improvement. You might expect that just from the difference in the size of these systems, but in AI computing that isn’t always the case.

!!! quote "DAVE SALVATOR, NVIDIA"

    If you just throw hardware at the problem, it’s not a given that you’re going to improve. We are getting essentially linear scaling.
    That represents a great achievement from our engineering teams.

Competitors are also getting closer to linear scaling. This round [Intel](https://spectrum.ieee.org/tag/intel) deployed a system using 1,024 GPUs that performed the GPT-3 task in 67 minutes versus a computer one-fourth the size that took 224 minutes six months ago. Google’s largest GPT-3 entry used 12-times the number of [TPU v5p](https://cloud.google.com/tpu/docs/v5p) accelerators as its smallest entry and performed its task nine times as fast.

Linear scaling is going to be particularly important for upcoming “AI factories” housing 100,000 GPUs or more, Salvator says. He says to expect one such data center to come online this year, and another, using Nvidia’s next architecture, [Blackwell](https://spectrum.ieee.org/nvidia-blackwell), to startup in 2025.

## Nvidia’s streak continues

Nvidia continued to boost training times despite using the same architecture, Hopper, as it did in last year’s training results. That’s all down to software improvements, says Salvator.

!!! quote "DAVE SALVATOR, NVIDIA"

    Typically, we’ll get a 2-2.5x [boost] fom software after a new architecture is released.

![minutes to train](../images/minute.png)

For GPT-3 training, Nvidia logged a 27 percent improvement from the [June 2023 MLPerf benchmarks](https://spectrum.ieee.org/large-language-models-training-benchmark). Salvator says there were several software changes behind the boost. For example, Nvidia engineers tuned up Hopper’s use of less accurate, 8-bit floating point operations by trimming unnecessary conversions between 8-bit and 16-bit numbers and better targeting of which layers of a neural network could use the lower precision number format. They also found a more intelligent way to adjust the power budget of each chip’s compute engines, and sped communication among GPUs in a way that Salvator likened to “buttering your toast while it’s still in the toaster.”

Additionally, the company implemented a scheme called [flash attention](https://github.com/Dao-AILab/flash-attention). Invented in the Stanford University laboratory of Samba Nova founder Chris Re, flash attention is an algorithm that speeds transformer networks by minimizing writes to memory. When it first [showed up](https://spectrum.ieee.org/mlperf-rankings-2022) in MLPerf benchmarks, flash attention shaved as much as 10 percent from training times. (Intel, too, used a version of flash attention but not for GPT-3. It instead used the algorithm for one of the new benchmarks, fine-tuning.)

Using other software and network tricks, Nvidia delivered an 80 percent speedup in the text-to-image test, Stable Diffusion, versus its submission in November 2023.

## New benchmarks

MLPerf adds new benchmarks and upgrades old ones to stay relevant to what’s happening in the AI industry. This year saw the addition of fine-tuning and graph neural networks.

Fine tuning takes an already trained LLM and specializes it for use in a particular field. Nvidia, for example took a trained 43-billion-parameter model and trained it on the GPU-maker’s design files and documentation to create [ChipNeMo, an AI intended to boost the productivity of its chip designers](https://spectrum.ieee.org/ai-for-engineering). At the time, the company’s chief technology officer Bill Dally said that training an LLM was like giving it a liberal arts education, and fine tuning was like sending it to graduate school.

The MLPerf benchmark takes a pretrained Llama-2-70B model and asks the system to fine tune it using a [dataset of government documents](https://huggingface.co/datasets/tau/scrolls) with the goal of generating more accurate document summaries.

There are several ways to do fine-tuning. MLPerf chose one called low-rank adaptation (LoRA). The method winds up training only a small portion of the LLM’s parameters leading to a 3-fold lower burden on hardware and reduced use of memory and storage versus other methods, according to the organization.

The other new benchmark involved a [graph neural network (GNN)](https://spectrum.ieee.org/machine-learning-in-physics). These are for problems that can be represented by a very large set of interconnected nodes, such as a social network or a recommender system. Compared to other AI tasks, GNNs require a lot of communication between nodes in a computer.

The benchmark trained a GNN on a database that shows relationships about academic authors, papers, and institutes—a graph with 547 million nodes and 5.8 billion edges. The neural network was then trained to predict the right label for each node in the graph.

## Future fights

Training rounds in 2025 may see head-to-head contests comparing new accelerators from [AMD](https://spectrum.ieee.org/tag/amd), Intel, and Nvidia. [AMD’s MI300 series](https://spectrum.ieee.org/amd-mi300) was launched about six months ago, and a memory-boosted upgrade the MI325x is [planned for the end of 2024](https://www.hpcwire.com/2024/06/03/amd-clears-up-messy-gpu-roadmap-upgrades-chips-annually/), with the next generation MI350 slated for 2025. [Intel says its Gaudi 3](https://spectrum.ieee.org/intel-gaudi-3), generally available to computer makers later this year, will appear in MLPerf’s upcoming inferencing benchmarks. Intel executives have said the new chip has the capacity to beat H100 at training LLMs. But the victory may be short-lived, as Nvidia has unveiled a new architecture, [Blackwell](https://spectrum.ieee.org/nvidia-blackwell), which is planned for late this year.
